{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOebnOXsu0HdQGaK0NydGPu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JcoMonkey/snakeGameDRL/blob/master/snakeGame_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uFkfA9M_W-Lp",
        "outputId": "68ff5c35-2bb8-46e6-a4a2-7900c6eecc34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'stable_baselines3'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2993805250.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgymnasium\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mA2C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMonitor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfigure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stable_baselines3'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import argparse\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import gymnasium as gym\n",
        "from stable_baselines3 import A2C\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.logger import configure\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback, CallbackList\n",
        "\n",
        "from snake_env import SnakeEnv\n",
        "\n",
        "#-- Helper function to create the environment ---\n",
        "def make_env(render_mode = None, reward_mode = \"length\", seed = 7):\n",
        "    env = SnakeEnv(render_mode = render_mode, reward_mode = reward_mode, seed = seed)\n",
        "    env = Monitor(env)\n",
        "    return env\n",
        "\n",
        "#-- Main function to train the entry point ---\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--timesteps\", type = int, default = 200_000)\n",
        "    parser.add_argument(\"--reward_mode\", type = str, default=\"length\", choices= [\"length\", \"survival\"])\n",
        "    parser.add_argument(\"--seed\", type = int, default = 7)\n",
        "    parser.add_argument(\"--logdir\", type = str, default = \"./logs\")\n",
        "    parser.add_argument(\"--modeldir\", type =str, default = \"./models\")\n",
        "    parser.add_argument(\"--results\", type = str, default = \"./results/reward_stats.json\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    os.makedirs(args.logdir, exist_ok = True)\n",
        "    os.makedirs(args.modeldir, exist_ok = True)\n",
        "    os.makedirs(os.path.dirname(args.results), exist_ok = True)\n",
        "\n",
        "    env = make_env(reward_mode = args.reward_mode, seed = args.seed)\n",
        "    eval_env = make_env(reward_mode = args.reward_mode, seed = args.seed + 100)\n",
        "\n",
        "    # --- A2c Model ---\n",
        "    model = A2C(\n",
        "        policy = \"MlpPolicy\",\n",
        "        env = env,\n",
        "        verbose = 1,\n",
        "        seed = args.seed,\n",
        "        tensorboard_log = \"./tensorboard_logs/\",\n",
        "        learning_rate = 7e-4,\n",
        "        n_steps = 5,\n",
        "        gamma = 0.99,\n",
        "        gae_lambda = 1.0,\n",
        "        ent_coef = 0.01,\n",
        "        vf_coef = 0.5,\n",
        "        max_grad_norm = 0.5,\n",
        "    )\n",
        "\n",
        "    #-- Logger setup ---\n",
        "    new_logger = configure(args.logdir, [\"stdout\", \"tensorboard\"])\n",
        "    model.set_logger(new_logger)\n",
        "\n",
        "    #-- Callbacks ---\n",
        "    checkpoint_callback = CheckpointCallback(\n",
        "        save_freq = 10000,\n",
        "        save_path = \"./checkpoints/\",\n",
        "        name_prefix = \"snake_a2c\",\n",
        "        save_replay_buffer = True,\n",
        "        save_vecnormalize = False\n",
        "    )\n",
        "\n",
        "    eval_callback = EvalCallback(\n",
        "        eval_env,\n",
        "        best_model_save_path = args.modeldir,\n",
        "        log_path = args.logdir,\n",
        "        eval_freq = 5000,\n",
        "        deterministic = True,\n",
        "        render = False,\n",
        "        n_eval_episodes = 5,\n",
        "        verbose = 1\n",
        "    )\n",
        "\n",
        "    callback_list = CallbackList([checkpoint_callback, eval_callback])\n",
        "\n",
        "    #-- Training with progress bar---\n",
        "    print(\"\\n Starting A2C training...\")\n",
        "    start_time = time.time()\n",
        "    model.learn(total_timesteps = args.timesteps, progress_bar = True, callback = callback_list)\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"\\n Training completed in {elapsed/60:.2f} minutes.\")\n",
        "\n",
        "    #-- Save model ---\n",
        "    save_name = f\"a2c_snake_{args.reward_mode}\"     # This is the base name\n",
        "    path = os.path.join(args.modeldir, save_name)   # This is the full path without the path\n",
        "    model.save(path)                                # Stable Baselines3 will add .zip\n",
        "    print(f\" Saved A2C model to {path}.zip\")\n",
        "\n",
        "    #-- Evaluating average reward after training ---\n",
        "    print(\"\\n Evaluating model performance...\")\n",
        "    episode_rewards = []\n",
        "    obs, _ = eval_env.reset()\n",
        "    for ep in range(10):\n",
        "        done, total_reward = False, 0\n",
        "        while not done:\n",
        "            action, _ = model.predict(obs, deterministic = True)\n",
        "            obs, reward, done, truncated, info = eval_env.step(action)\n",
        "            total_reward += reward\n",
        "            if done or truncated:\n",
        "                episode_rewards.append(total_reward)\n",
        "                obs, _ = eval_env.reset()\n",
        "                break\n",
        "\n",
        "    avg_reward = np.mean(episode_rewards)\n",
        "    print(f\" Average reward over 10 episodes: {avg_reward:.2f}\")\n",
        "\n",
        "    #-- Save reward statistics ---\n",
        "    results = {}\n",
        "    if os.path.exists(args.results):\n",
        "        with open(args.results, \"r\") as f:\n",
        "            try:\n",
        "                results = json.load(f)\n",
        "            except json.JSONDecodeError:\n",
        "                results = {}\n",
        "\n",
        "    # store results under the current reward mode key\n",
        "    results[args.reward_mode] = {\n",
        "        \"average_reward\": float(avg_reward),\n",
        "        \"episodes\": len(episode_rewards),\n",
        "        \"timestamps\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"timesteps\": args.timesteps,\n",
        "        \"train_time_minutes\": round(elapsed / 60, 2)\n",
        "    }\n",
        "\n",
        "    with open(args.results, \"w\") as f:\n",
        "        json.dump(results, f, indent = 4)\n",
        "\n",
        "    print(f\" Saved reward results to {args.results}\")\n",
        "    print(json.dumps(results, indent = 4))\n",
        "\n",
        "    #-- Close environments ---\n",
        "    env.close()\n",
        "    eval_env.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}